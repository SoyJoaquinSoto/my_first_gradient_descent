{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent hecho a mí manera\n",
    "### (o en otras palabras: *sin saber lo que hago*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos del gradient descent:\n",
    " 1. Derivada de la loss function para cada parámetro (**gradient** (diferentes derivadas de una sola función) **of the loss function**).\n",
    " 2. Asignarle valores aleatorios a los parámetros.\n",
    " 3. Evaluar los valores de los parámetros en las derivadas (**gradient**).\n",
    " 4. Calcular el tamaño de los pasos: **step size = parameter * learning rate**.\n",
    " 5. Calcular el valor nuevo de los parámetros: **new parameter value = old parameter value - step size**.\n",
    " 6. Repetir desde el paso 3 hasta que se llegue al número máximo de pasos (>= 1000) o se llegue al tamaño mínimo de paso (<= 0.001).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de las librerías\n",
    "import sympy as sym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la función de gradient_descent\n",
    "def gradient_descent(data, learning_rate, min_step_size, max_num_steps):\n",
    "    num_steps = 0\n",
    "    \n",
    "    # Definición de la loss function\n",
    "    loss_function, intercept, slope, observed_x, observed_y = sym.symbols('loss_function intercept slope observed_x observed_y')\n",
    "    loss_function = (observed_y-(intercept+slope*observed_x))**2\n",
    "\n",
    "    # Derivada de la loss function con respecto a intercept\n",
    "    der_intercept = sym.symbols('der_intercept')\n",
    "    der_intercept = sym.diff(loss_function, intercept)\n",
    "\n",
    "    # Derivada de la loss function con respecto a slope\n",
    "    der_slope = sym.symbols('der_slope')\n",
    "    der_slope = sym.diff(loss_function, slope)\n",
    "\n",
    "    # Asignación de valores aleatorios a intercept y slope\n",
    "    intercept_value, slope_value = np.random.rand(2)\n",
    "\n",
    "    # Repetir mientras el tamaño de los pasos sea mayor al mínimo \n",
    "    # y el número de pasos menor al máximo\n",
    "    while True:\n",
    "        # Reiniciar el valor de la variable de la sumatoria\n",
    "        sum_slope_intercept = 0\n",
    "        sum_slope_slope = 0\n",
    "\n",
    "        # Evaluar el valor de intercept y slope para cada punto\n",
    "        for x, y in data:\n",
    "            sum_slope_intercept += der_intercept.subs([(intercept, intercept_value), (slope, slope_value), (observed_x, x), (observed_y, y)])\n",
    "            sum_slope_slope += der_slope.subs([(intercept, intercept_value), (slope, slope_value), (observed_x, x), (observed_y, y)])\n",
    "\n",
    "        # Calcular los nuevos tamaños de los pasos\n",
    "        step_size_intercept = sum_slope_intercept * learning_rate\n",
    "        step_size_slope = sum_slope_slope * learning_rate\n",
    "\n",
    "        # Calcular los nuevos valores de intercept y de slope\n",
    "        intercept_value = intercept_value - step_size_intercept\n",
    "        slope_value = slope_value - step_size_slope\n",
    "\n",
    "        # Se aumenta en uno el número de pasos\n",
    "        num_steps += 1\n",
    "        \n",
    "        # Condición para terminar los cálculos\n",
    "        if abs(step_size_intercept) <= min_step_size and abs(step_size_slope) <= min_step_size or num_steps >= max_num_steps:\n",
    "            return intercept_value, slope_value, num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecuación de regresión calculada después de  294  pasos:\n",
      "0.644699333774921*x + 0.940095392378976\n"
     ]
    }
   ],
   "source": [
    "# Creación del dataset\n",
    "data = np.array([[0.5, 1.4], [2.3, 1.9], [2.9, 3.2]])\n",
    "\n",
    "# Creación de las variables de aprendizaje\n",
    "learning_rate = 0.01\n",
    "min_step_size = 0.0001\n",
    "max_num_steps = 1000\n",
    "        \n",
    "# Impresión del resultado\n",
    "intercept, slope, num_steps = gradient_descent(data, learning_rate, min_step_size, max_num_steps)\n",
    "regression_equation, x = sym.symbols('regression_equation x')\n",
    "regression_equation = slope*x + intercept\n",
    "print(\"Ecuación de regresión calculada después de \", num_steps, \" pasos:\")\n",
    "print(str(regression_equation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
